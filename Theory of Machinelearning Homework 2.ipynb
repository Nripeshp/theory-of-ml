{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 : Loss minimization vs. misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QixR3EzkhQKY"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,w):\n",
    "    tmp = (math.e)**(-x*y*w)\n",
    "    tmp1 = 1/(1+tmp)\n",
    "    return tmp*tmp1*(-y)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findw(w,dataPoints):\n",
    "    for i in range(100):\n",
    "        g = 0\n",
    "        for j in dataPoints:\n",
    "            g = g + gradientDescent(j[0],j[1],w)\n",
    "        g = g/100\n",
    "        w = w - (0.1)*g\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Consider the case d = 1 and implement gradient descent for minimizing L(w). First consider a\n",
    "simple “well separable” case with 100 data points (and labels) as follows:\n",
    "(−50,−1), (−49,−1), ..., (−1,−1), (1,1), (2,1), ..., (50,1).\n",
    "Initialize w = −1 and run 100 iterations of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WX5cc5NxkW9J",
    "outputId": "883d1768-4020-4adf-ad3f-c132b1e96cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6056835787730888\n"
     ]
    }
   ],
   "source": [
    "dataPoints = []\n",
    "w = -1\n",
    "for i in range(1,51):\n",
    "    dataPoints.append((-i,-1))\n",
    "    dataPoints.append((i,1))\n",
    "\n",
    "print(findw(w,dataPoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Now suppose we “corrupt” some labels. Specifically, take the 10 points with the highest abso-\n",
    "lute value of x (i.e., 50, −50, 49, −49, ..., 46, −46) and reverse the sign of their labels. Now\n",
    "show the result of performing gradient descent, and interpret your result. (Note that we only\n",
    "corrupted 10% of the labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JovMt7Tno-Q",
    "outputId": "24565522-ebe3-4206-c089-d33503000442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8461383595192684\n"
     ]
    }
   ],
   "source": [
    "dataPoints = []\n",
    "w = -1\n",
    "for i in range(1,51):\n",
    "    if i<46:\n",
    "        dataPoints.append((-i,-1))\n",
    "        dataPoints.append((i,1))   \n",
    "    else:\n",
    "        dataPoints.append((i,-1))\n",
    "        dataPoints.append((-i,1))\n",
    "        \n",
    "answer = findw(w,dataPoints)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSet(m,n):\n",
    "    arr = [[0 for i in range(n)] for j in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            num = random.uniform(-1.0,1.0)\n",
    "            arr[i][j] = num\n",
    "    return arr\n",
    "A = randomSet(1000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Create a ‘random’ dataset for this problem as follows. Set n = 500, m = 2n, and let A have\n",
    "entries that are random in the interval [−1,1]. Now choose some “hidden” vector x∗ (entries\n",
    "again random in [−1,1]), and define b as Ax∗ + η, where η is a vector whose coordinates are\n",
    "Gaussian with mean 0 and variance 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomx():\n",
    "    arr = [[0 for i in range(1)] for j in range(500)]\n",
    "    for i in range(500):\n",
    "        for j in range(1):\n",
    "            num = random.uniform(-1.0,1.0)\n",
    "            arr[i][j] = num\n",
    "    return arr\n",
    "x = randomx()\n",
    "n = np.random.normal(loc=0.0, scale=math.sqrt(0.5), size=1000)\n",
    "AT = np.transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEzzDu5JCkIr",
    "outputId": "174a1b0e-efc0-4e75-fcb5-fd99c15ab6a0"
   },
   "outputs": [],
   "source": [
    "def find_b():\n",
    "    b = [0 for i in range(len(n))]\n",
    "    for i in range(len(A)):\n",
    "        tmp = 0\n",
    "        for j in range(len(A[i])):\n",
    "            tmp += (A[i][j]*x[j][0])\n",
    "        b[i] = tmp + n[i]\n",
    "    return b\n",
    "b = find_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Graduent Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(inp):\n",
    "    return 2*np.subtract(np.dot(np.dot(AT,A),inp),np.dot(AT,b))\n",
    "\n",
    "def gradient_descent(inp,step_size,n):\n",
    "    mat = inp\n",
    "    for _ in range(n):\n",
    "        mat +=  -step_size * gradient(mat)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Run gradient descent with a constant stepsize (say 1/10) starting with x0 = 0, and report the\n",
    "function value and the distance to the ‘hidden’ x∗ after 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.2058558420000054\n",
      "Value: 3.1256006167962924e+230\n",
      "Distance from hidden x 1.2887328832751707e+115\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "init = [0 for i in range(len(x))]\n",
    "ans = gradient_descent(init,0.1,50)\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time: \"+str(stop - start))\n",
    "\n",
    "tm = n[:]\n",
    "for i in range(len(A)):\n",
    "    tmp = 0\n",
    "    for j in range(len(A[i])):\n",
    "        tmp += (A[i][j]*ans[j])\n",
    "    tm[i] = tmp + b[i]\n",
    "print(\"Value: \" + str(np.linalg.norm(tm)**2))\n",
    "\n",
    "dist = np.linalg.norm(x-ans)\n",
    "print(\"Distance from hidden x \" + str(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) It is well known that least squares regression has a closed form, given by x∗ = (AT A)−1AT b.\n",
    "Using a numerical library for the inverse, compute x∗ using this formula. Compare the running\n",
    "time of this method to that of gradient descent in part (d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.06084523999999192\n",
      "Value:161.2006930256866\n",
      "Distance from hidden x*402.17505529999346\n"
     ]
    }
   ],
   "source": [
    "def closed_form():\n",
    "    tmp = np.dot(np.linalg.inv(np.dot(AT,A)),AT)\n",
    "    ans =  np.dot(tmp,b) \n",
    "    return ans\n",
    "\n",
    "start = timeit.default_timer()\n",
    "closed= closed_form()\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time: \"+str(stop - start))\n",
    "print(\"Value:\" + str(np.linalg.norm(closed)**2))\n",
    "print(\"Distance from hidden x*\" + str(np.linalg.norm(x-closed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UVtyhqru_hv"
   },
   "source": [
    "# Problem 4 : Convergence of SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implement the SGD algorithm for this f. For concreteness, start with x0 = y0 = 1. Observe the\n",
    "values of yt for 200 steps with a constant learning rate (say 0.1) and comment on your results.\n",
    "What happens if you change the learning rate to 0.1/(t + 1) and 0.1/√t + 1? (Note that these\n",
    "are time-dependent learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qTMnRIAdu_xs"
   },
   "outputs": [],
   "source": [
    "def learningRate1(x,y):\n",
    "    for t in range(1,201):\n",
    "        grad_a = 0\n",
    "        grad_b = 0\n",
    "        for i in range(1,101):\n",
    "            if i<=50:\n",
    "                a = i/100\n",
    "                b = -1\n",
    "            else:\n",
    "                a = (i-100)/100\n",
    "                b = 1\n",
    "            grad_a += (2*(x-a))\n",
    "            grad_b += (2*(y-b))\n",
    "        x = x - (0.1)*(grad_a/200)\n",
    "        y = y - (0.1)*(grad_b/200)\n",
    "        #print(\"step \"+str(i)+\" :\"+str(y))\n",
    "    print(\"For 0.1/: x = \" + str(x) +\" and \"+ \" y = \" + str(y) )\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kED4ewTFu_5y"
   },
   "outputs": [],
   "source": [
    "def learningRate2(x,y):\n",
    "    for t in range(1,201):\n",
    "        A = 0\n",
    "        B = 0\n",
    "        for i in range(1,101):\n",
    "            if i<=50:\n",
    "                a = i/100\n",
    "                b = -1\n",
    "            else:\n",
    "                a = (i-100)/100\n",
    "                b = 1\n",
    "            A = A + (2*(x-a))\n",
    "            B = B + (2*(y-b))\n",
    "        x = x - (0.1/t+1)*(A/200)\n",
    "        y = y - (0.1/t+1)*(B/200)\n",
    "    print('For 0.1/(t+1) : x = '+ str(x) +\" and \"+ \" y = \" + str(y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vGORQbv0-Atw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def learningRate3(x,y):\n",
    "    for t in range(1,201):\n",
    "        A = 0\n",
    "        B = 0\n",
    "        for i in range(1,101):\n",
    "            if i<=50:\n",
    "                a = i/100\n",
    "                b = -1\n",
    "            else:\n",
    "                a = (i-100)/100\n",
    "                b = 1\n",
    "            A = A + (2*(x-a))\n",
    "            B = B + (2*(y-b))\n",
    "        x = x - (0.1/(math.sqrt(t+1))*(A/200))\n",
    "        y = y - (0.1/(math.sqrt(t+1))*(B/200))\n",
    "    print('For 0.1/√(t+1) : x = '+ str(x) +\" and \"+ \" y = \" + str(y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKyaFBBu-Gop",
    "outputId": "ea066bbc-ca70-4135-ca78-0fb3313f9f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0.1/: x = 0.005000000701980365 and  y = 7.05507775727842e-10\n",
      "For 0.1/(t+1) : x = 0.004999999999999982 and  y = 1.4697461203939076e-17\n",
      "For 0.1/√(t+1) : x = 0.07759102595798106 and  y = 0.07295580498289558\n"
     ]
    }
   ],
   "source": [
    "learningRate1(1,1)\n",
    "learningRate2(1,1)\n",
    "learningRate3(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TOM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
